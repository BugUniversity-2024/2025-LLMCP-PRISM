"""
Prompt Engine - è´Ÿè´£ç”Ÿæˆç»“æ„åŒ– Prompt
é˜¶æ®µ 1: Mock å®ç°
é˜¶æ®µ 2: æ¥å…¥ OpenAI GPT-4o
"""
import json
import random
import time
from pathlib import Path
from typing import Dict, Any


# System Prompt æ¨¡æ¿ï¼ˆé˜¶æ®µ 2 ä½¿ç”¨ï¼‰
GENERATION_SYSTEM_PROMPT = """ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ AI ç»˜ç”» Prompt ç”Ÿæˆå™¨ï¼Œæ“…é•¿å°†ç”¨æˆ·çš„ç®€å•æè¿°è½¬æ¢ä¸ºè¯¦ç»†ã€ç»“æ„åŒ–çš„ç»˜ç”»æŒ‡ä»¤ã€‚

ã€è¾“å‡ºæ ¼å¼ã€‘
ä½ å¿…é¡»è¾“å‡º JSON æ ¼å¼ï¼ŒåŒ…å«ä»¥ä¸‹å­—æ®µï¼š
{
  "subject": ["ä¸»ä½“æè¿°"],
  "appearance": ["å¤–è§‚ç»†èŠ‚"],
  "style": ["ç”»é£é£æ ¼"],
  "composition": ["æ„å›¾æ–¹å¼"],
  "lighting": ["å…‰ç…§æè¿°"],
  "background": ["èƒŒæ™¯æè¿°"],
  "quality": ["è´¨é‡è¦æ±‚"],
  "negative": ["è´Ÿé¢æç¤º"],
  "weights": {"style": 1.0, "realism": 0.7}
}

ã€ç”Ÿæˆè§„åˆ™ã€‘
1. æ ¹æ®ç”¨æˆ·è¾“å…¥å¡«å……æ‰€æœ‰å­—æ®µï¼Œä¿è¯å®Œæ•´æ€§
2. ä½¿ç”¨ä¸“ä¸šç»˜ç”»æœ¯è¯­ï¼ˆå¦‚ï¼šrim lightã€bokehã€cinematic compositionã€soft cel-shadingï¼‰
3. é¿å…å†²çªï¼ˆä¸èƒ½åŒæ—¶è¦æ±‚ realistic å’Œ cartoonï¼‰
4. ä½¿ç”¨ä¸­æ–‡æè¿°ï¼Œé€‚é…å›¾åƒç”Ÿæˆæ¨¡å‹
5. å¦‚æœç”¨æˆ·è¾“å…¥ç®€å•ï¼Œåˆç†è¡¥å……ç»†èŠ‚ï¼ˆä½†ä¸åç¦»ä¸»é¢˜ï¼‰

ã€å‚è€ƒæ¡ˆä¾‹ã€‘
æ¡ˆä¾‹1ï¼ˆç§‹æ—¥è‰åœ°ä¸‰äººåœºæ™¯ï¼‰ï¼š
ç”¨æˆ·è¾“å…¥ï¼š"ä¸‰ä¸ªäººèººåœ¨è‰åœ°ä¸Šçœ‹è½å¶"
Schema è¾“å‡ºï¼š
{
  "subject": ["ä¸‰ä½è§’è‰²", "å¤´å¯¹å¤´èººåœ¨è‰åœ°"],
  "appearance": ["é¢éƒ¨æ¸…æ™°", "æœè£…è‡ªç„¶", "å¤´å‘éšé£æ•£å¼€"],
  "style": ["äºŒæ¬¡å…ƒåŠå†™å®", "æ˜æ˜¾çº¿æ¡æ„Ÿ", "çœŸå®å…‰å½±"],
  "composition": ["ä¿¯æ‹70åº¦", "åœ†å½¢æ„å›¾", "å¤´éƒ¨å±…ä¸­"],
  "lighting": ["æ¸©æš–åˆåå…‰", "ä¾§é€†å…‰", "æŸ”å’Œé«˜å…‰"],
  "background": ["ç§‹å¤©è‰åœ°", "é»„ç»¿è¤è‰²", "è½å¶é£˜è½"],
  "quality": ["16:9", "1920x1080", "é«˜æ¸…ç»†è…»"],
  "negative": ["æ¨¡ç³Š", "å˜å½¢", "è¿‡åº¦æ‰å¹³"],
  "weights": {"style": 1.0, "realism": 0.8}
}

æ¡ˆä¾‹2ï¼ˆç«™å°åœºæ™¯ï¼‰ï¼š
ç”¨æˆ·è¾“å…¥ï¼š"ä¸€ä¸ªäººååœ¨åœ°é“ç«™å°è¾¹ç¼˜ï¼Œé›¾æ°”å¼¥æ¼«"
Schema è¾“å‡ºï¼š
{
  "subject": ["ä¸€ä½è§’è‰²ååœ¨ç«™å°è¾¹ç¼˜"],
  "appearance": ["åŠ¨æ¼«çº¿æ¡", "æŸ”å’Œä¸Šè‰²", "æ²‰é™è¡¨æƒ…"],
  "style": ["ç”µå½±æ„Ÿ", "åŠå†™å®èƒŒæ™¯", "åŠ¨æ¼«è§’è‰²"],
  "composition": ["å¹³è§†", "ä¸­è·ç¦»", "å¯¹é¢è§†è§’"],
  "lighting": ["æ¸…æ™¨æ·¡é‡‘è“æ··åˆ", "ä½“ç§¯é›¾", "æŸ”å…‰"],
  "background": ["åœ°é“ç«™å°", "è½»å¾®é›¾æ°”", "è½¨é“å»¶ä¼¸"],
  "quality": ["16:9", "é«˜æ¸…", "æµ…æ™¯æ·±"],
  "negative": ["æ‹¥æŒ¤", "ç§‘å¹»UI", "å¤¸å¼ æ¯”ä¾‹"],
  "weights": {"style": 1.0, "realism": 0.6}
}

ã€é‡è¦æç¤ºã€‘
- ä¿æŒæ‰€æœ‰æè¿°ä¸ºä¸­æ–‡
- ç¡®ä¿ JSON æ ¼å¼æ­£ç¡®
- æ¯ä¸ªæ•°ç»„è‡³å°‘æœ‰1-3ä¸ªå…ƒç´ 
- weights çš„å€¼åœ¨ 0.1-1.5 ä¹‹é—´
"""


class PromptEngine:
    """Prompt ç”Ÿæˆå¼•æ“ï¼ˆé˜¶æ®µ 1: Mock å®ç°ï¼‰"""

    def __init__(self, use_real_api: bool = False):
        """
        Args:
            use_real_api: æ˜¯å¦ä½¿ç”¨çœŸå® OpenAI APIï¼ˆé˜¶æ®µ 2 è®¾ç½®ä¸º Trueï¼‰
        """
        self.use_real_api = use_real_api
        if use_real_api:
            from openai import OpenAI
            from app.config import settings
            self.client = OpenAI(
                api_key=settings.openai_api_key,
                base_url=settings.openai_api_base  # æ”¯æŒè‡ªå®šä¹‰ Base URL
            )
            self.model = settings.openai_model

        # åŠ è½½ System Promptï¼ˆå¦‚æœä½¿ç”¨çœŸå® APIï¼‰
        if use_real_api:
            self.system_prompt = self._load_system_prompt("generation.txt")

    def _load_system_prompt(self, filename: str) -> str:
        """ä» prompts ç›®å½•åŠ è½½ System Prompt"""
        prompt_path = Path(__file__).parent.parent / "prompts" / filename
        try:
            with open(prompt_path, "r", encoding="utf-8") as f:
                return f.read()
        except FileNotFoundError:
            print(f"âš ï¸ Prompt æ–‡ä»¶æœªæ‰¾åˆ°ï¼š{prompt_path}ï¼Œä½¿ç”¨é»˜è®¤ prompt")
            return GENERATION_SYSTEM_PROMPT

    def generate_schema(self, user_input: str) -> Dict[str, Any]:
        """
        æ ¹æ®ç”¨æˆ·è¾“å…¥ç”Ÿæˆç»“æ„åŒ– Schema

        Args:
            user_input: ç”¨æˆ·çš„åˆ›æ„æè¿°

        Returns:
            {
                "schema": dict,  # ç»“æ„åŒ– Schema
                "prompt": str    # è‡ªç„¶è¯­è¨€ Prompt
            }

        Raises:
            ValueError: ç”¨æˆ·è¾“å…¥ä¸ºç©º
            RuntimeError: OpenAI API è°ƒç”¨å¤±è´¥
        """
        if not user_input or not user_input.strip():
            raise ValueError("ç”¨æˆ·è¾“å…¥ä¸èƒ½ä¸ºç©º")

        if self.use_real_api:
            return self._generate_with_openai(user_input)
        else:
            return self._generate_mock(user_input)

    def _generate_mock(self, user_input: str) -> Dict[str, Any]:
        """é˜¶æ®µ 1: Mock å®ç°"""
        # é¢„è®¾çš„ä¸¤ä¸ª Schema æ¨¡æ¿
        templates = [
            {
                "subject": ["ä¸€åªæ©˜çŒ«", "åå§¿"],
                "appearance": ["æ©˜è‰²æ¯›å‘", "è“è‰²çœ¼ç›", "è“¬æ¾å°¾å·´"],
                "style": ["åŠå†™å®", "åŠ¨æ¼«é£æ ¼", "æŸ”å’Œçº¿æ¡"],
                "composition": ["ç‰¹å†™", "æµ…æ™¯æ·±", "æ­£é¢è§†è§’"],
                "lighting": ["æŸ”å’Œä¾§å…‰", "æš–è‰²è°ƒ", "æ—¥è½å…‰"],
                "background": ["çª—è¾¹", "æ—¥è½", "æœ¦èƒ§èƒŒæ™¯"],
                "quality": ["é«˜æ¸…", "ç»†èŠ‚ä¸°å¯Œ", "16:9"],
                "negative": ["æ¨¡ç³Š", "å˜å½¢", "å¤šä½™è‚¢ä½“"],
                "weights": {"style": 1.0, "realism": 0.7}
            },
            {
                "subject": ["ä¸‰ä½è§’è‰²", "å¤´å¯¹å¤´èººåœ¨è‰åœ°"],
                "appearance": ["é¢éƒ¨æ¸…æ™°", "æœè£…è‡ªç„¶", "å¤´å‘éšé£æ•£å¼€"],
                "style": ["äºŒæ¬¡å…ƒåŠå†™å®", "æ˜æ˜¾çº¿æ¡æ„Ÿ", "çœŸå®å…‰å½±"],
                "composition": ["ä¿¯æ‹70åº¦", "åœ†å½¢æ„å›¾", "å¤´éƒ¨å±…ä¸­"],
                "lighting": ["æ¸©æš–åˆåå…‰", "ä¾§é€†å…‰", "æŸ”å’Œé«˜å…‰"],
                "background": ["ç§‹å¤©è‰åœ°", "é»„ç»¿è¤è‰²", "è½å¶é£˜è½"],
                "quality": ["16:9", "1920x1080", "é«˜æ¸…ç»†è…»"],
                "negative": ["æ¨¡ç³Š", "å˜å½¢", "è¿‡åº¦æ‰å¹³"],
                "weights": {"style": 1.0, "realism": 0.8}
            }
        ]

        # éšæœºé€‰æ‹©ä¸€ä¸ªæ¨¡æ¿
        schema = random.choice(templates)

        # æ¸²æŸ“ä¸º Prompt
        prompt = self._render_prompt(schema)

        return {
            "schema": schema,
            "prompt": prompt
        }

    def _generate_with_openai(self, user_input: str) -> Dict[str, Any]:
        """é˜¶æ®µ 2: çœŸå® OpenAI API è°ƒç”¨ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰"""
        max_retries = 3
        retry_delay = 1  # ç§’

        for attempt in range(max_retries):
            try:
                print(f"ğŸ”„ è°ƒç”¨ OpenAI API (å°è¯• {attempt + 1}/{max_retries})...")

                response = self.client.chat.completions.create(
                    model=self.model,
                    messages=[
                        {"role": "system", "content": self.system_prompt},
                        {"role": "user", "content": user_input}
                    ],
                    response_format={"type": "json_object"},
                    temperature=0.7,
                    max_tokens=1500
                )

                schema_json = json.loads(response.choices[0].message.content)

                # éªŒè¯ Schema å®Œæ•´æ€§
                self._validate_schema(schema_json)

                prompt = self._render_prompt(schema_json)

                print(f"âœ… Prompt ç”ŸæˆæˆåŠŸ")
                return {
                    "schema": schema_json,
                    "prompt": prompt
                }

            except json.JSONDecodeError as e:
                print(f"âš ï¸ Schema è§£æå¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    continue
                else:
                    print(f"âŒ è§£æå¤±è´¥ï¼Œå›é€€åˆ° mock æ¨¡å¼")
                    return self._generate_mock(user_input)

            except Exception as e:
                print(f"âš ï¸ OpenAI API è°ƒç”¨å¤±è´¥: {e}")
                if attempt < max_retries - 1:
                    time.sleep(retry_delay)
                    retry_delay *= 2  # æŒ‡æ•°é€€é¿
                    continue
                else:
                    print(f"âŒ API è°ƒç”¨å¤±è´¥ï¼Œå›é€€åˆ° mock æ¨¡å¼")
                    return self._generate_mock(user_input)

    def _validate_schema(self, schema: Dict[str, Any]):
        """éªŒè¯ Schema å®Œæ•´æ€§"""
        required_fields = [
            "subject", "appearance", "style", "composition",
            "lighting", "background", "quality", "negative", "weights"
        ]
        for field in required_fields:
            if field not in schema:
                raise ValueError(f"Schema ç¼ºå°‘å¿…è¦å­—æ®µï¼š{field}")

    def _render_prompt(self, schema: Dict[str, Any]) -> str:
        """
        å°† Schema æ¸²æŸ“ä¸ºè‡ªç„¶è¯­è¨€ Prompt
        """
        parts = []

        # 1. ç”»é¢æ¯”ä¾‹å’Œè´¨é‡
        if schema.get("quality"):
            parts.append(f"ç”»é¢æ¯”ä¾‹ï¼š16:9ï¼Œ{', '.join(schema['quality'])}")

        # 2. é£æ ¼è¦æ±‚
        if schema.get("style"):
            parts.append(f"é£æ ¼è¦æ±‚ï¼š{', '.join(schema['style'])}")

        # 3. ä¸»ä½“åœºæ™¯ä¸æ„å›¾
        parts.append("\nã€ä¸»ä½“åœºæ™¯ä¸æ„å›¾ã€‘")
        if schema.get("subject"):
            parts.append(f"ä¸»ä½“ï¼š{', '.join(schema['subject'])}")
        if schema.get("composition"):
            parts.append(f"æ„å›¾ï¼š{', '.join(schema['composition'])}")

        # 4. å¤–è§‚ç»†èŠ‚
        if schema.get("appearance"):
            parts.append("\nã€å¤–è§‚ç»†èŠ‚ã€‘")
            parts.append(', '.join(schema['appearance']))

        # 5. å…‰ç…§ä¸æ°›å›´
        if schema.get("lighting"):
            parts.append("\nã€å…‰ç…§ä¸æ°›å›´ã€‘")
            parts.append(', '.join(schema['lighting']))

        # 6. èƒŒæ™¯
        if schema.get("background"):
            parts.append("\nã€èƒŒæ™¯ã€‘")
            parts.append(', '.join(schema['background']))

        # 7. è´Ÿé¢æç¤º
        if schema.get("negative"):
            parts.append("\nã€è´Ÿå‘æç¤ºï¼ˆé¿å…ï¼‰ã€‘")
            parts.append(', '.join(schema['negative']))

        return '\n'.join(parts)
